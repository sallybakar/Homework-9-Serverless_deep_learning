{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1VdzGTSpdANEw3hO42GHfg7AAMtnkPpa-","timestamp":1735830619902}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!wget https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle/model_2024_hairstyle.keras"],"metadata":{"id":"gjWN19OwokSZ","outputId":"d24d83f8-f1bb-46e1-b337-bc1635084f75","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735853516557,"user_tz":-60,"elapsed":1989,"user":{"displayName":"Abubakar Salimon","userId":"11177653108751824839"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-01-02 21:31:52--  https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle/model_2024_hairstyle.keras\n","Resolving github.com (github.com)... 140.82.114.3\n","Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/426348925/df5735c1-9082-4b67-968e-866f268793f8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250102%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250102T213152Z&X-Amz-Expires=300&X-Amz-Signature=0de4c9646b205b681e5c8e5121a543b94165b3868985953579f1099d6ef3a8a6&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmodel_2024_hairstyle.keras&response-content-type=application%2Foctet-stream [following]\n","--2025-01-02 21:31:52--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/426348925/df5735c1-9082-4b67-968e-866f268793f8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250102%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250102T213152Z&X-Amz-Expires=300&X-Amz-Signature=0de4c9646b205b681e5c8e5121a543b94165b3868985953579f1099d6ef3a8a6&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmodel_2024_hairstyle.keras&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 160610502 (153M) [application/octet-stream]\n","Saving to: ‘model_2024_hairstyle.keras’\n","\n","model_2024_hairstyl 100%[===================>] 153.17M   132MB/s    in 1.2s    \n","\n","2025-01-02 21:31:53 (132 MB/s) - ‘model_2024_hairstyle.keras’ saved [160610502/160610502]\n","\n"]}]},{"cell_type":"code","source":["!pip install tensorflow==2.17.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QQUofcY66gVp","executionInfo":{"status":"ok","timestamp":1735853529374,"user_tz":-60,"elapsed":4153,"user":{"displayName":"Abubakar Salimon","userId":"11177653108751824839"}},"outputId":"ae9645cc-452e-49a9-da9c-22d4a6b45253"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow==2.17.1 in /usr/local/lib/python3.10/dist-packages (2.17.1)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (3.12.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (18.1.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (0.4.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (4.25.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (1.17.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (1.68.1)\n","Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (2.17.1)\n","Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (3.5.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (0.37.1)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.1) (1.26.4)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.17.1) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.1) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.1) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.1) (0.13.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.1) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.1) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.1) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.1) (2024.12.14)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.1) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.1) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.1) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow==2.17.1) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow==2.17.1) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow==2.17.1) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow==2.17.1) (0.1.2)\n"]}]},{"cell_type":"markdown","source":["# Question 1\n","\n","Now convert this model from Keras to TF-Lite format.\n","\n","What's the size of the converted model?\n","\n","+ 27 Mb\n","+ 43 Mb\n","+ 77 Mb\n","+ 127 Mb"],"metadata":{"id":"8ACV3H9SkXdS"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FORIFEbZjyT1","executionInfo":{"status":"ok","timestamp":1735853546269,"user_tz":-60,"elapsed":7188,"user":{"displayName":"Abubakar Salimon","userId":"11177653108751824839"}},"outputId":"a7aa002a-cb73-44e5-e9b5-4de4596f599a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tflite-runtime\n","  Downloading tflite_runtime-2.14.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.10/dist-packages (from tflite-runtime) (1.26.4)\n","Downloading tflite_runtime-2.14.0-cp310-cp310-manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tflite-runtime\n","Successfully installed tflite-runtime-2.14.0\n"]}],"source":["!pip install tflite-runtime"]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","# Load the Keras model\n","model = keras.models.load_model(\"/content/model_2024_hairstyle.keras\")\n","#model = tf.keras.models.load_model('model_2024_hairstyle.keras')\n","\n","# Convert the model to tf-lite\n","converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","tflite_model = converter.convert()\n","\n","\n","# Save the tf-lite model to disk\n","with tf.io.gfile.GFile('hair-model.tflite', 'wb') as f:\n","    f.write(tflite_model)\n","#with open('model_2024_hairstyle.tflite', 'wb') as f:\n","    #f.write(tflite_model)\n","\n","# Print the size of the converted model\n","#import os\n","#print(f\"Size of the converted model: {os.path.getsize('model_2024_hairstyle.tflite') / 1024 / 1024:.2f} MB\")"],"metadata":{"id":"NfloMF-9oTID","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735855578970,"user_tz":-60,"elapsed":5417,"user":{"displayName":"Abubakar Salimon","userId":"11177653108751824839"}},"outputId":"abf58c55-ad2b-4ec4-c8b9-289981d1e112"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved artifact at '/tmp/tmp6rfkzmhf'. The following endpoints are available:\n","\n","* Endpoint 'serve'\n","  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 200, 200, 3), dtype=tf.float32, name='input_layer')\n","Output Type:\n","  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n","Captures:\n","  136484129576512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  136483924449712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  136483924449536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  136483925055920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  136483925060672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  136483925057504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"]}]},{"cell_type":"code","source":["!ls -l -h | grep \".tflite\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"maivvVKUpEi2","executionInfo":{"status":"ok","timestamp":1735855583791,"user_tz":-60,"elapsed":491,"user":{"displayName":"Abubakar Salimon","userId":"11177653108751824839"}},"outputId":"ed6ab5b3-637b-44ca-d160-93489d5e630d"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["-rw-r--r-- 1 root root  77M Jan  2 22:06 hair-model.tflite\n","-rw-r--r-- 1 root root  77M Jan  2 21:57 model_2024_hairstyle.tflite\n"]}]},{"cell_type":"markdown","source":["Size of the converted model: 77 MB"],"metadata":{"id":"n-23jSXu_sLD"}},{"cell_type":"markdown","source":["# Question 2\n","To be able to use this model, we need to know the index of the input and the index of the output.\n","\n","What's the output index for this model?\n","\n","+ 3\n","+ 7\n","+ 13\n","+ 24"],"metadata":{"id":"LpxTgUt3qY6B"}},{"cell_type":"code","source":["import tensorflow.lite as tflite\n","\n","# Load the TFLite model and allocate tensors\n","interpreter = tflite.Interpreter(model_path='hair-model.tflite')\n","interpreter.allocate_tensors()\n","\n","# Get input tensors\n","input_details = interpreter.get_input_details()\n","#input_index = input_details[0]['index']\n","\n","# Get output tensors\n","output_details = interpreter.get_output_details()\n","#output_index = output_details[0]['index']\n","\n","#input_index, output_index\n","\n","# Get indices of the input and output tensors\n","print(\"Input index:\", input_details[0]['index'])\n","print(\"Output index:\", output_details[0]['index'])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZkzJggEero6z","executionInfo":{"status":"ok","timestamp":1735856827138,"user_tz":-60,"elapsed":442,"user":{"displayName":"Abubakar Salimon","userId":"11177653108751824839"}},"outputId":"0cd508e6-757b-4b9a-8bef-aaad0d003c70"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Input index: 0\n","Output index: 13\n"]}]},{"cell_type":"markdown","source":["# Preparing the image\n","You'll need some code for downloading and resizing images. You can use this code:"],"metadata":{"id":"CRXY2yAS0Uwt"}},{"cell_type":"code","source":["from io import BytesIO\n","from urllib import request\n","\n","from PIL import Image\n","\n","def download_image(url):\n","    with request.urlopen(url) as resp:\n","        buffer = resp.read()\n","    stream = BytesIO(buffer)\n","    img = Image.open(stream)\n","    return img\n","\n","def prepare_image(img, target_size):\n","    if img.mode != 'RGB':\n","        img = img.convert('RGB')\n","    img = img.resize(target_size, Image.NEAREST)\n","    return img"],"metadata":{"id":"O_y5GI240UKG","executionInfo":{"status":"ok","timestamp":1735853589366,"user_tz":-60,"elapsed":488,"user":{"displayName":"Abubakar Salimon","userId":"11177653108751824839"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["For that, you'll need to have `pillow` installed:"],"metadata":{"id":"3k83Ymni0v4k"}},{"cell_type":"code","source":["!pip install pillow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QwkSY6UN0rQC","executionInfo":{"status":"ok","timestamp":1735853599688,"user_tz":-60,"elapsed":3594,"user":{"displayName":"Abubakar Salimon","userId":"11177653108751824839"}},"outputId":"5c2f8246-f879-4571-8a65-2dfde222135f"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n"]}]},{"cell_type":"markdown","source":["Let's download and resize this image:\n","\n","[https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg](https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg)\n","\n","Based on the previous homework, what should be the target size for the image?\n","\n","# Question 3\n","Now we need to turn the image into numpy array and pre-process it.\n","\n","> Tip: Check the previous homework. What was the pre-processing we did there?\n","\n","After the pre-processing, what's the value in the first pixel, the R channel?\n","\n","+ 0.24\n","+ 0.44\n","+ 0.64\n","+ 0.84"],"metadata":{"id":"jvCrPuQl1Npw"}},{"cell_type":"code","source":["import numpy as np\n","\n","def prepare_input(x):\n","    return x / 255.0\n","\n","URL = \"https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\"\n","img = download_image(URL)\n","img = prepare_image(img, target_size=(200, 200))\n","\n","# Assuming 'img' is loaded and resized to the target size\n","X = np.array([img], dtype=np.float32)\n","X = prepare_input(X)\n","#round(X[0, 0, 0, 0], 2)\n","print(\"First pixel, R channel value:\", round(X[0, 0, 0, 0], 2))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"06vYTu3Y1M-Z","executionInfo":{"status":"ok","timestamp":1735856437377,"user_tz":-60,"elapsed":1312,"user":{"displayName":"Abubakar Salimon","userId":"11177653108751824839"}},"outputId":"081516d1-10c2-429d-bf34-e5f1364ede76"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["First pixel, R channel value: 0.24\n"]}]},{"cell_type":"markdown","source":["First pixel, R channel value: 0.24"],"metadata":{"id":"3DXP4l9_EK1G"}},{"cell_type":"markdown","source":["# Question 4\n","Now let's apply this model to this image. What's the output of the model?\n","\n","+ 0.293\n","+ 0.493\n","+ 0.693\n","+ 0.893"],"metadata":{"id":"ehYzij2P6uJW"}},{"cell_type":"code","source":["# Set the input tensor\n","interpreter.set_tensor(input_index, X)\n","\n","# Run inference\n","interpreter.invoke()\n","\n","# Get the output\n","preds = interpreter.get_tensor(output_index)\n","preds[0,0]\n","print(\"Model output:\", preds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ak79VI4d6sHc","executionInfo":{"status":"ok","timestamp":1735856319944,"user_tz":-60,"elapsed":443,"user":{"displayName":"Abubakar Salimon","userId":"11177653108751824839"}},"outputId":"e8d9294a-0af3-43c2-f6b3-4805191596f6"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Model output: [[0.8937741]]\n"]}]},{"cell_type":"markdown","source":["# Prepare the lambda code\n","Now you need to copy all the code into a separate python file. You will need to use this file for the next two questions.\n","\n","Tip: you can test this file locally with ipython or Jupyter Notebook by importing the file and invoking the function from this file.\n","\n","# Docker\n","For the next two questions, we'll use a Docker image that we already prepared. This is the Dockerfile that we used for creating the image:\n","```\n","FROM public.ecr.aws/lambda/python:3.10\n","\n","COPY model_2024_hairstyle_v2.tflite .\n","\n","RUN pip install numpy==1.23.1\n","```\n","Note that it uses Python 3.10. The latest models of TF Lite do not support Python 3.12 yet, so we need to use 3.10. Also, for this part, we will use TensorFlow 2.14.0. We have tested it, and the models created with 2.17 could be served with 2.14.0.\n","\n","For that image, we also needed to use an older version of numpy (1.23.1)\n","\n","The docker image is published to [agrigorev/model-2024-hairstyle:v3](https://hub.docker.com/r/agrigorev/model-2024-hairstyle/tags/).\n","\n","A few notes:\n","\n","The image already contains a model and it's not the same model as the one we used for questions 1-4.\n","The wheel for this combination that you'll need to use in your Docker image is [https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl](https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl)"],"metadata":{"id":"4gcXIhZy8oSP"}},{"cell_type":"markdown","source":["# Question 5\n","Download the base image `agrigorev/model-2024-hairstyle:v3`. You can do it with docker pull.\n","\n","So what's the size of this base image?\n","\n","+ 182 Mb\n","+ 382 Mb\n","+ 582 Mb\n","+ 782 Mb\n","\n","You can get this information when running `docker images` - it'll be in the \"SIZE\" column."],"metadata":{"id":"gZaSM99EEfi1"}},{"cell_type":"code","source":["# Copyright 2024 Drengskapur\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","#\n","# @title {display-mode:\"form\"}\n","# @markdown <br/><br/><center><img src=\"https://cdn.jsdelivr.net/gh/drengskapur/docker-in-colab/assets/docker.svg\" height=\"150\"><img src=\"https://cdn.jsdelivr.net/gh/drengskapur/docker-in-colab/assets/colab.svg\" height=\"150\"></center><br/>\n","# @markdown <center><h1>Docker in Colab</h1></center><center>github.com/drengskapur/docker-in-colab<br/><br/><br/><b>udocker(\"run hello-world\")</b></center><br/>\n","def udocker_init():\n","    import os\n","    if not os.path.exists(\"/home/user\"):\n","        !pip install udocker > /dev/null\n","        !udocker --allow-root install > /dev/null\n","        !useradd -m user > /dev/null\n","    print(f'Docker-in-Colab 1.1.0\\n')\n","    print(f'Usage:     udocker(\"--help\")')\n","    print(f'Examples:  https://github.com/indigo-dc/udocker?tab=readme-ov-file#examples')\n","\n","    def execute(command: str):\n","        user_prompt = \"\\033[1;32muser@pc\\033[0m\"\n","        print(f\"{user_prompt}$ udocker {command}\")\n","        !su - user -c \"udocker $command\"\n","\n","    return execute\n","\n","udocker = udocker_init()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pU8MEwG5A3Jw","executionInfo":{"status":"ok","timestamp":1735857101931,"user_tz":-60,"elapsed":477,"user":{"displayName":"Abubakar Salimon","userId":"11177653108751824839"}},"outputId":"d27ab9e2-cb02-4706-b48c-f175392ef381"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Docker-in-Colab 1.1.0\n","\n","Usage:     udocker(\"--help\")\n","Examples:  https://github.com/indigo-dc/udocker?tab=readme-ov-file#examples\n"]}]},{"cell_type":"code","source":["udocker(\"pull agrigorev/model-2024-hairstyle:v3\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4N0JNr_QC2VV","executionInfo":{"status":"ok","timestamp":1735853677339,"user_tz":-60,"elapsed":14434,"user":{"displayName":"Abubakar Salimon","userId":"11177653108751824839"}},"outputId":"b75646ec-0261-4715-faec-7924e86948d3"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;32muser@pc\u001b[0m$ udocker pull agrigorev/model-2024-hairstyle:v3\n","Info: creating repo: /home/user/.udocker\n","Info: udocker command line interface 1.3.17\n","Info: searching for udockertools >= 1.2.11\n","Info: installing udockertools 1.2.11\n","Info: installation of udockertools successful\n","Info: downloading layer sha256:607850a2138e9b1752d8e93bad8c9a94b3caa2f282a5b04eefaff27bdcf2ef4f\n","Info: downloading layer sha256:5642750232a55f9c840fd98caa18aa62b0ce5e17854931200cfd369a4155e96c\n","Info: downloading layer sha256:b14de35356a056edfc7853c7ef5c47e81b7784e761615d48b52dccd54b385cab\n","Info: downloading layer sha256:719d0580071dac4699262ccb182f95cfe2c395a60293e1cbcae509a639520030\n","Info: downloading layer sha256:6f5f8c0b748710ed16f9bb208d914bb0d999fc39d65e3255cdaee7eea1312593\n","Info: downloading layer sha256:79a77e7c1be9a2c4f77ead609e8d8b7162377bb6905b2a244c7964d74d8c8762\n","Info: downloading layer sha256:e6c48b038848595d63aa080f2c6f49bce4f3388cab8a35257dbb8d6ca99fcab5\n","Info: downloading layer sha256:999c124cce46bef524436003cfa2cb41a59324d54baf8e76b3575e3fe9bfe4b8\n","Info: downloading layer sha256:299668a79b8add698f8396eda6413971b0a555ba3ae46e38e2e70c0874e09449\n"]}]},{"cell_type":"code","source":["udocker(\"create agrigorev/model-2024-hairstyle:v3\")\n","udocker(\"ps -m -s\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BHtRl_wODCQO","executionInfo":{"status":"ok","timestamp":1735857184489,"user_tz":-60,"elapsed":19876,"user":{"displayName":"Abubakar Salimon","userId":"11177653108751824839"}},"outputId":"debdbbd5-0430-40a3-bb19-82dc7487ba8d"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;32muser@pc\u001b[0m$ udocker create agrigorev/model-2024-hairstyle:v3\n","884efaba-c045-3109-9aa5-1bdae89723ec\n","\u001b[1;32muser@pc\u001b[0m$ udocker ps -m -s\n","CONTAINER ID                         P M MOD SIZE NAMES              IMAGE               \n","5aad0d45-4eda-361b-9b0e-07d6e890738c . W P1   805                    agrigorev/model-2024-hairstyle:v3\n","efff19c0-afd2-3080-9704-36647574f0c7 . W P1   805                    agrigorev/model-2024-hairstyle:v3\n","884efaba-c045-3109-9aa5-1bdae89723ec . W P1   805                    agrigorev/model-2024-hairstyle:v3\n"]}]},{"cell_type":"markdown","source":["<img src=\"https://i.postimg.cc/28BW4YSP/imagen-2024-12-09-173116559.png\">"],"metadata":{"id":"k65hA-vBHqjZ"}},{"cell_type":"markdown","source":["The size of the image is  782MB\n","     "],"metadata":{"id":"QbAUcBa6JpGQ"}},{"cell_type":"markdown","source":["# Question 6\n","Now let's extend this docker image, install all the required libraries and add the code for lambda.\n","\n","You don't need to include the model in the image. It's already included. The name of the file with the model is `model_2024_hairstyle_v2.tflite` and it's in the current workdir in the image (see the Dockerfile above for the reference). The provided model requires the same preprocessing for images regarding target size and rescaling the value range than used in homework 8.\n","\n","Now run the container locally.\n","\n","Score this image: https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\n","\n","What's the output from the model?\n","\n","+ 0.229\n","+ 0.429\n","+ 0.629\n","+ 0.829"],"metadata":{"id":"TwukcsmVInBp"}},{"cell_type":"markdown","source":["<img src=\"https://i.postimg.cc/xjBNJBF1/Captura-de-pantalla-2024-12-17-081409.png\">"],"metadata":{"id":"BdgTQUJfVEte"}},{"cell_type":"markdown","source":["The output of the model is [0.42985352873802185]"],"metadata":{"id":"htlkk3llJ_H3"}}]}